{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zx158/anaconda3/envs/new_torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# model loading\n",
    "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\") # token=access_token\n",
    "\n",
    "# tokenizer loading\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False) # token=access_token\n",
    "tokenizer.pad_token_id = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 64 # 64 for real test\n",
    "path_test_set = \"../representation-engineering/data/MMLU/data/test/elementary_mathematics_test.csv\" # change this to your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(lst, batch_size):\n",
    "    \"\"\"Yield successive batch_size chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "def decompose_df(df):\n",
    "    # get the question, and let each question replicate 4 times\n",
    "    df_4_times = df.loc[df.index.repeat(4)].reset_index(drop=True)\n",
    "    questions = (\"Question: \" + df_4_times.iloc[:,0]).values.tolist()\n",
    "    # get the choices\n",
    "    choices = df_4_times.iloc[:,1:5].values.tolist()\n",
    "    # get each choice once\n",
    "    choices = [\"Answer: \" + choices[i][i % 4] for i in range(len(choices))] \n",
    "\n",
    "    # use the original df to construct labels\n",
    "    labels = df.iloc[:,5].values.tolist()\n",
    "    # turn labels from \"A, B, C, D\" to \"0, 1, 2, 3\" using lambda function\n",
    "    labels = list(map(lambda x: ord(x) - ord('A'), labels))\n",
    "\n",
    "    print(\"Example question: {}\".format(questions[0:5]))\n",
    "    print(\"Example choice: {}\".format(choices[0:5]))\n",
    "    print(\"Example label: {}\".format(labels[0:5]))\n",
    "\n",
    "    return questions, choices, labels\n",
    "\n",
    "def load_mmlu_sentences(path_test_set):\n",
    "    df = pd.read_csv(path_test_set, header=None)\n",
    "    questions, choices, labels = decompose_df(df)\n",
    "    return questions, choices, labels\n",
    "\n",
    "# need further look into this\n",
    "def get_logprobs(logits, input_ids, masks, **kwargs):\n",
    "    logprobs = F.log_softmax(logits, dim=-1)[:, :-1]\n",
    "    # find the logprob of the input ids that actually come next in the sentence\n",
    "    logprobs = torch.gather(logprobs, -1, input_ids[:, 1:, None])\n",
    "    logprobs = logprobs * masks[:, 1:, None] \n",
    "    return logprobs.squeeze(-1)\n",
    "\n",
    "def calc_acc(labels, output_logprobs):\n",
    "    output_logprobs = np.array(output_logprobs).reshape(-1, 4)\n",
    "    model_answer = np.argmax(output_logprobs, axis=1)\n",
    "    correct = model_answer == labels\n",
    "    return correct.mean(), model_answer\n",
    "\n",
    "def prepare_decoder_only_inputs(prompts, targets, tokenizer, device):\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    prompt_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    target_inputs = tokenizer(targets, return_tensors=\"pt\", padding=True, truncation=False, add_special_tokens=False)\n",
    "    \n",
    "    # concatenate prompt and target tokens and send to device\n",
    "    inputs = {k: torch.cat([prompt_inputs[k], target_inputs[k]], dim=1).to(device) for k in prompt_inputs}\n",
    "    # print(\"decoder only inputs: {}\".format(inputs))\n",
    "\n",
    "    # mask is zero for padding tokens\n",
    "    mask = inputs[\"attention_mask\"].clone()\n",
    "    # set mask to 0 for question tokens\n",
    "    mask[:, :prompt_inputs[\"input_ids\"].shape[1]] = 0\n",
    "    mask.to(device)\n",
    "    # remove token_type_ids\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "    \n",
    "    return inputs, mask, prompt_inputs[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example question: ['Question: What is the value of p in 24 = 2p?', 'Question: What is the value of p in 24 = 2p?', 'Question: What is the value of p in 24 = 2p?', 'Question: What is the value of p in 24 = 2p?', 'Question: Ms. Perez drove a total of 40 miles in 5 days. She drove the same number of miles each day. How many miles did Ms. Perez drive each day?']\n",
      "Example choice: ['Answer: p = 4', 'Answer: p = 8', 'Answer: p = 12', 'Answer: p = 24', 'Answer: 5']\n",
      "Example label: [2, 2, 3, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:13,  1.76it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.585\n",
      "model answer example: [2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluation(model, tokenizer, questions, answers, labels, batch_size=128):\n",
    "    gc.collect()\n",
    "\n",
    "    model.eval()\n",
    "    output_logprobs = []\n",
    "    for q_batch, a_batch in tqdm(zip(batchify(questions, batch_size), batchify(answers, batch_size)), total=len(questions)//batch_size):\n",
    "        inputs, masks, _ = prepare_decoder_only_inputs(q_batch, a_batch, tokenizer, model.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # set the masks so that we do not add to tokens of input sentences and padding tokens\n",
    "                model.set_masks(masks.unsqueeze(-1))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # calculate the probabilities for all tokens (all question answer pairs)\n",
    "            logits = model(**inputs).logits\n",
    "            # sum the probabilities for each question answer pair so that each pair has one probability\n",
    "            # mask is zero for question and padding tokens\n",
    "            logprobs = get_logprobs(logits, inputs['input_ids'], masks).sum(-1).detach().cpu().numpy()\n",
    "        output_logprobs.extend(logprobs)\n",
    "\n",
    "    return calc_acc(labels, output_logprobs)\n",
    "    \n",
    "questions, choices, labels = load_mmlu_sentences(path_test_set)\n",
    "acc, model_answer = evaluation(model, tokenizer, questions, choices, labels, batch_size=batch_size)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(\"model answer example: {}\".format(model_answer[0:2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

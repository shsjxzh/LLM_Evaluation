{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zx158/anaconda3/envs/new_torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# model loading\n",
    "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\") # token=access_token\n",
    "\n",
    "# tokenizer loading\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False) # token=access_token\n",
    "tokenizer.pad_token_id = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 64 # 64 for real test\n",
    "path_test_set = \"../../data/MMLU/data/test/elementary_mathematics_test.csv\" # change this to your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(lst, batch_size):\n",
    "    \"\"\"Yield successive batch_size chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "def decompose_df(df):\n",
    "    inputs = (\"Question: \" + df.iloc[:,0] + \" Choice: A: \" + df.iloc[:,1] + \", B: \" + df.iloc[:,2] + \", C: \" + df.iloc[:,3] + \", D: \" + df.iloc[:,4] + \". Please provide only the choice and answer as succinctly as possible. Answer:\").values.tolist()\n",
    "    print(inputs[0:2])\n",
    "    answers = df.iloc[:,5].values.tolist()\n",
    "    print(\"Example answer: {}\".format(answers[0:2]))\n",
    "    # turn answers from \"A, B, C, D\" to \"0, 1, 2, 3\" using lambda function\n",
    "    answers = list(map(lambda x: ord(x) - ord('A'), answers))\n",
    "    print(\"Answer in number: {}\".format(answers[0:2]))\n",
    "    return inputs, answers\n",
    "\n",
    "def load_mmlu_sentences(path_test_set):\n",
    "    df = pd.read_csv(path_test_set, header=None)\n",
    "    inputs, answers = decompose_df(df)\n",
    "    return inputs, answers\n",
    "\n",
    "def prepare_decoder_only_inputs(inputs, tokenizer):\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    packed_inputs = tokenizer(inputs, padding=True, truncation=False, return_tensors=\"pt\") # , return_attention_mask=True\n",
    "\n",
    "    # remove token_type_ids to save space\n",
    "    if \"token_type_ids\" in packed_inputs:\n",
    "        del packed_inputs[\"token_type_ids\"]\n",
    "\n",
    "    return packed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Question: What is the value of p in 24 = 2p? Choice: A: p = 4, B: p = 8, C: p = 12, D: p = 24. Please provide only the choice and answer as succinctly as possible. Answer:', 'Question: Ms. Perez drove a total of 40 miles in 5 days. She drove the same number of miles each day. How many miles did Ms. Perez drive each day? Choice: A: 5, B: 7, C: 8, D: 9. Please provide only the choice and answer as succinctly as possible. Answer:']\n",
      "Answer: ['C', 'C']\n",
      "Answer in number: [2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:09,  1.54s/it]                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer example: [1 2]\n",
      "Ground truth answer example: [2, 2]\n",
      "Accuracy: 0.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluation(model, tokenizer, inputs, answers, batch_size=128):\n",
    "    gc.collect()\n",
    "\n",
    "    model.eval()\n",
    "    # For choice tokens, we want both \" A\" and \"A\".\n",
    "    # the tokens to be extracted depends on the separator used by the tokenizer\n",
    "    # see https://huggingface.co/docs/transformers/v4.18.0/en/tokenizer_summary\n",
    "    # and \"SentencePiece\" for more details\n",
    "    tokens_be_extracted = ['▁A', '▁B', '▁C', '▁D', 'A', 'B', 'C', 'D']\n",
    "    tokens_index = [tokenizer.convert_tokens_to_ids(token) for token in tokens_be_extracted]\n",
    "\n",
    "    # get the log probability of all the samples\n",
    "    log_probs_extracted_all = []\n",
    "    for input_batch in tqdm(batchify(inputs, batch_size), total=len(inputs)//batch_size):\n",
    "        packed_inputs = prepare_decoder_only_inputs(input_batch, tokenizer)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**packed_inputs).logits\n",
    "            # use the next token's logits\n",
    "            logits = logits[:,-1,:].squeeze()\n",
    "            # derive the probability corresponding to \"A\", \"B\", \"C\", \"D\"\n",
    "            log_logits = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            log_probs_extracted = log_logits[:, tokens_index]\n",
    "            log_probs_extracted_all.append(log_probs_extracted)\n",
    "\n",
    "    # calculate the accuracy based on the log probability\n",
    "    log_probs_extracted_all = torch.cat(log_probs_extracted_all, dim=0)\n",
    "    choice_all = log_probs_extracted_all.cpu().numpy().argmax(axis=1)\n",
    "    # print(choice_all.shape)\n",
    "    print(\"model answer example: {}\".format(choice_all[0:2]))\n",
    "    # Here we mod 4 because we take both \" A\" and \"A\" into account.\n",
    "    choice_all %= 4\n",
    "    print(\"Ground truth answer example: {}\".format(answers[:2]))\n",
    "    acc = (choice_all == answers[:len(choice_all)]).mean()\n",
    "    return acc\n",
    "\n",
    "inputs, answers = load_mmlu_sentences(path_test_set)\n",
    "acc = evaluation(model, tokenizer, inputs, answers, batch_size=batch_size)\n",
    "print(f\"Accuracy: {acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
